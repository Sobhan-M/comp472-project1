{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Words As Features\n",
    "In this part we will use a count vectorizer to determine our features. We will then use those features to train several models using different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will re-import the information, as seen in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('goemotions.json') as f:\n",
    "\tdataset = np.array(json.load(f))\n",
    "\t\n",
    "posts = dataset[:, 0]\n",
    "emotions = dataset[:, 1]\n",
    "sentiments = dataset[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Extraction\n",
    "We will extract the features (vocabulary in each post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizedPosts = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the size of our vocabulary through 2 approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30449\n",
      "(171820, 30449)\n"
     ]
    }
   ],
   "source": [
    "# 1. Vectorizer features.\n",
    "print(len(vectorizer.get_feature_names_out()))\n",
    "\n",
    "# 2. Vectorized post shape (the column number).\n",
    "print(vectorizedPosts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now also convert our classes into numerical values so we can use them. For this, we will use a LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Emotions.\n",
    "emotionsLabelEncoder = LabelEncoder()\n",
    "encodedEmotions = emotionsLabelEncoder.fit_transform(emotions)\n",
    "\n",
    "# Sentiments.\n",
    "sentimentsLabelEncoder = LabelEncoder()\n",
    "encodedSentiments = sentimentsLabelEncoder.fit_transform(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test to see if we have correctly encoded our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emotions: 28\n",
      "Number of sentiments: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of emotions: {len(emotionsLabelEncoder.classes_)}\")\n",
    "print(f\"Number of sentiments: {len(sentimentsLabelEncoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Splitting For Training And Testing\n",
    "We will use built in functions to split our data for training and testing (for both sentiments and emotions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "postsForEmotions_train, postsForEmotions_test, emotions_train, emotions_test =  train_test_split(vectorizedPosts, encodedEmotions, test_size=0.20)\n",
    "postsForSentiments_train, postsForSentiments_test, sentiments_train, sentiments_test =  train_test_split(vectorizedPosts, encodedSentiments, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Training And Testing\n",
    "In this section we will train and test various models with different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 - Base Multinomial Naive Bayes\n",
    "We simply use `MultinomialNB` to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "baseMNBClassifier_emotions = MultinomialNB()\n",
    "baseMNBClassifier_sentiments = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit, predict, and test our emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3847631241997439"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseMNBClassifier_emotions.fit(postsForEmotions_train, emotions_train)\n",
    "baseMNB_emotions_results = baseMNBClassifier_emotions.predict(postsForEmotions_test)\n",
    "baseMNBClassifier_emotions.score(postsForEmotions_test, emotions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the process for the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5456000465603539"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseMNBClassifier_sentiments.fit(postsForSentiments_train, sentiments_train)\n",
    "baseMNB_sentiments_results = baseMNBClassifier_sentiments.predict(postsForSentiments_test)\n",
    "baseMNBClassifier_sentiments.score(postsForSentiments_test, sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 - Base Decision Tree\n",
    "We will be using the `DecisionTreeClassifier` with default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "baseDTClassifier_emotions = DecisionTreeClassifier()\n",
    "baseDTClassifier_sentiments = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting, predicting, and testing the emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35572110348038644"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseDTClassifier_emotions.fit(postsForEmotions_train, emotions_train)\n",
    "baseDT_emotions_results = baseDTClassifier_emotions.predict(postsForEmotions_test)\n",
    "baseDTClassifier_emotions.score(postsForEmotions_test, emotions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the process for sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5463566523105575"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseDTClassifier_sentiments.fit(postsForSentiments_train, sentiments_train)\n",
    "baseDT_sentiments_results = baseDTClassifier_sentiments.predict(postsForSentiments_test)\n",
    "baseDTClassifier_sentiments.score(postsForSentiments_test, sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 - Base Multi-Layered Perceptron\n",
    "We will use the MLPClassifier by SciKit. The only change is that we will use the `early_stopping` hyper parameter. Without setting this to true, the model does not converge (I have waited over 35 minutes without success). This is partially due to my hardware restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "baseMLPClassifier_emotions = MLPClassifier(early_stopping=True)\n",
    "baseMLPClassifier_sentiments = MLPClassifier(early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train, predict, and test our emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43475730415551156"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseMLPClassifier_emotions.fit(postsForEmotions_train, emotions_train)\n",
    "baseMLP_emotions_results = baseMLPClassifier_emotions.predict(postsForEmotions_test)\n",
    "baseMLPClassifier_emotions.score(postsForEmotions_test, emotions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the process for the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5744674659527412"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseMLPClassifier_sentiments.fit(postsForSentiments_train, sentiments_train)\n",
    "baseMLP_sentiments_results = baseMLPClassifier_sentiments.predict(postsForSentiments_test)\n",
    "baseMLPClassifier_sentiments.score(postsForSentiments_test, sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 - Top Multinomial Bayes Classifier\n",
    "We use `GridSearchCV` to find the optimal hyperparameters for our Multinomial Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "topMNBParameters = {\n",
    "    'alpha': (1,0.5,0.25,0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train, predict, and test the emotions using the top classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39104877197066695"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topMNBsearch_emotions = GridSearchCV(baseMNBClassifier_emotions, topMNBParameters)\n",
    "topMNBsearch_emotions.fit(postsForEmotions_train, emotions_train)\n",
    "topMNB_emotions_results = topMNBsearch_emotions.predict(postsForEmotions_test)\n",
    "topMNBsearch_emotions.score(postsForEmotions_test, emotions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then repeat the process for the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "/home/sobhan/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5456000465603539"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topMNBsearch_sentiments = GridSearchCV(baseMNBClassifier_sentiments, topMNBParameters)\n",
    "topMNBsearch_sentiments.fit(postsForSentiments_train, sentiments_train)\n",
    "topMNB_sentiments_results = topMNBsearch_sentiments.predict(postsForSentiments_test)\n",
    "topMNBsearch_sentiments.score(postsForSentiments_test, sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 - Top Decision Tree\n",
    "We repeat a similar process to the previous part, but for our Decision Tree instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help decide our max depth options, we will first look at the current max-depth to get an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions Max Depth: 1575\n",
      "Sentiments Max Depth: 1378\n"
     ]
    }
   ],
   "source": [
    "print(f\"Emotions Max Depth: {baseDTClassifier_emotions.tree_.max_depth}\")\n",
    "print(f\"Sentiments Max Depth: {baseDTClassifier_sentiments.tree_.max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "topDTParameters = {\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'max_depth': (400,800),\n",
    "    'min_samples_split': (4,6,8)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train, predict, and test the emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37175532534047256"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topDTsearch_emotions = GridSearchCV(baseDTClassifier_emotions, topDTParameters)\n",
    "topDTsearch_emotions.fit(postsForEmotions_train, emotions_train)\n",
    "topDT_emotions_results = topDTsearch_emotions.predict(postsForEmotions_test)\n",
    "topDTsearch_emotions.score(postsForEmotions_test, emotions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the process for the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5489174717727855"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topDTsearch_sentiments = GridSearchCV(baseDTClassifier_sentiments, topDTParameters)\n",
    "topDTsearch_sentiments.fit(postsForSentiments_train, sentiments_train)\n",
    "topDT_sentiments_results = topDTsearch_sentiments.predict(postsForSentiments_test)\n",
    "topDTsearch_sentiments.score(postsForSentiments_test, sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 -Top Multi-Layered Perceptron\n",
    "We can repeat the process for our multi-layered perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "topMLPParameters = {\n",
    "    'activation': ('logistic', 'tanh', 'relu', 'identity'),\n",
    "    'hidden_layer_sizes': ((30,50), (10,10,10)),\n",
    "    'solver': ('sgd', 'adam')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train, predict, and test the emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4315562798277267"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topMLPsearch_emotions = GridSearchCV(baseMLPClassifier_emotions, topMLPParameters)\n",
    "topMLPsearch_emotions.fit(postsForEmotions_train, emotions_train)\n",
    "topMLP_emotions_results = topMLPsearch_emotions.predict(postsForEmotions_test)\n",
    "topMLPsearch_emotions.score(postsForEmotions_test, emotions_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then repeat the process for the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topMLPsearch_sentiments = GridSearchCV(baseMLPClassifier_sentiments, topMLPParameters)\n",
    "topMLPsearch_sentiments.fit(postsForSentiments_train, sentiments_train)\n",
    "topMLP_sentiments_results = topMLPsearch_sentiments.predict(postsForSentiments_test)\n",
    "topMLPsearch_sentiments.score(postsForSentiments_test, sentiments_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Performance\n",
    "We will be writing the values of the different models to `performance`. This will include their metrics, confusion matrices, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def stringifyConfusionMatrix(confusionMatrix):\n",
    "\toutput = \"\"\n",
    "\n",
    "\tfor row in confusionMatrix:\n",
    "\t\tfor column in row:\n",
    "\t\t\toutput += f\"{column}\\t\"\n",
    "\t\toutput += \"\\n\"\n",
    "\t\n",
    "\treturn output\n",
    "\n",
    "def logPerformance(title, emotionsActual, emotionsPredicted, sentimentsActual, sentimentsPredicted):\n",
    "\twith open('performance.txt', 'a') as outfile:\n",
    "\t\toutfile.write(\"\\n# {title}\\n\")\n",
    "\t\toutfile.write(\"## Emotions:\\n\")\n",
    "\t\toutfile.write(\"### Confusion Matrix:\\n\")\n",
    "\t\toutfile.write(stringifyConfusionMatrix(metrics.confusion_matrix(emotionsActual, emotionsPredicted)))\n",
    "\t\toutfile.write(\"\\n### Metrics:\\n\")\n",
    "\t\toutfile.write(metrics.classification_report(emotionsActual, emotionsPredicted))\n",
    "\t\toutfile.write(\"## Sentiments:\\n\")\n",
    "\t\toutfile.write(\"### Confusion Matrix:\\n\")\n",
    "\t\toutfile.write(stringifyConfusionMatrix(metrics.confusion_matrix(sentimentsActual, sentimentsPredicted)))\n",
    "\t\toutfile.write(\"\\n### Metrics:\\n\")\n",
    "\t\toutfile.write(metrics.classification_report(sentimentsActual, sentimentsPredicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now log the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Base Multinomial Naive Bayes\n",
    "logPerformance(\"Base Multinomial Naive Bayes\", emotions_test, baseMNB_emotions_results, sentiments_test, baseMNB_sentiments_results)\n",
    "\n",
    "# For Base Decision Tree\n",
    "logPerformance(\"Base Decision Tree\", emotions_test, baseDT_emotions_results, sentiments_test, baseDT_sentiments_results)\n",
    "\n",
    "# For Base Multi-Layered Perceptron\n",
    "logPerformance(\"Base Multi-Layered Perceptron\", emotions_test, baseMLP_emotions_results, sentiments_test, baseMLP_sentiments_results)\n",
    "\n",
    "# For Top Multinomial Naive Bayes\n",
    "logPerformance(\"Top Multinomial Naive Bayes\", emotions_test, topMNB_emotions_results, sentiments_test, topMNB_sentiments_results)\n",
    "\n",
    "# For Top Decision Tree\n",
    "logPerformance(\"Top Decision Tree\", emotions_test, topDT_emotions_results, sentiments_test, topDT_sentiments_results)\n",
    "\n",
    "# For Top Multi-Layered Perceptron\n",
    "# logPerformance(\"Top Multi-Layered Perceptron\", emotions_test, topMLP_emotions_results, sentiments_test, topMLP_sentiments_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Self Exploration\n",
    "I will be doing the self-exploration regarding training sets. Please refer to `part2-exploration-1.ipynb` and `part2-exploration-2.ipynb` for this part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
